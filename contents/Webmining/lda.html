
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Pemodelan Topik LDA &#8212; Data Science Materials</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Data Science Materials</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    Materi tentang data science: webmining, statistik, dan datamining
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Webmining
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="seleksifitur.html">
   Seleksi Fitur Chi-squre
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chisquare.html">
   Implementasi Seleksi fitur Chi-Square
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chi.html">
   Chi-square menggunakan R
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="topikmodelinng.html">
   Topic Modeling
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Statistik
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Cipomean.html">
   Pendugaa/Estimasi Parameter Populasi
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ConfidenceIntervaltwopopulationproportions.html">
   Interval Kepercayaan untuk selisih antara dua Proporsi
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/mulaab/saindata"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/mulaab/saindata/issues/new?title=Issue%20on%20page%20%2Fcontents/Webmining/lda.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../_sources/contents/Webmining/lda.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Pemodelan Topik  LDA
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#https-petamind-com-understanding-latent-dirichlet-allocation-lda">
   https://petamind.com/understanding-latent-dirichlet-allocation-lda/
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pemodelan-topik">
     2. Pemodelan Topik
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#apa-itu-topic-modeling">
       2.1. Apa itu  Topic Modeling?
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#applications">
       2.2. Applications
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#teknik-pemodelan-topik">
       2.3. Teknik Pemodelan Topik
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#latent-dirichlet-allocation">
     3. Latent Dirichlet Allocation
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#introduction">
       3.1. Introduction
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#kelebihan-dan-kekurangan-dari-lda">
     Kelebihan dan kekurangan dari LDA
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#latent-dirichlet-allocation-lda">
     <strong>
      LATENT DIRICHLET ALLOCATION (LDA)
     </strong>
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Pemodelan Topik  LDA</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Pemodelan Topik  LDA
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#https-petamind-com-understanding-latent-dirichlet-allocation-lda">
   https://petamind.com/understanding-latent-dirichlet-allocation-lda/
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pemodelan-topik">
     2. Pemodelan Topik
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#apa-itu-topic-modeling">
       2.1. Apa itu  Topic Modeling?
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#applications">
       2.2. Applications
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#teknik-pemodelan-topik">
       2.3. Teknik Pemodelan Topik
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#latent-dirichlet-allocation">
     3. Latent Dirichlet Allocation
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#introduction">
       3.1. Introduction
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#kelebihan-dan-kekurangan-dari-lda">
     Kelebihan dan kekurangan dari LDA
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#latent-dirichlet-allocation-lda">
     <strong>
      LATENT DIRICHLET ALLOCATION (LDA)
     </strong>
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="pemodelan-topik-lda">
<h1>Pemodelan Topik  LDA<a class="headerlink" href="#pemodelan-topik-lda" title="Permalink to this headline">#</a></h1>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="https-petamind-com-understanding-latent-dirichlet-allocation-lda">
<h1><a class="reference external" href="https://petamind.com/understanding-latent-dirichlet-allocation-lda/">https://petamind.com/understanding-latent-dirichlet-allocation-lda/</a><a class="headerlink" href="#https-petamind-com-understanding-latent-dirichlet-allocation-lda" title="Permalink to this headline">#</a></h1>
<p><a class="reference external" href="https://medium.com/analytics-vidhya/topic-modeling-with-latent-dirichlet-allocation-lda-196c287e221">https://medium.com/analytics-vidhya/topic-modeling-with-latent-dirichlet-allocation-lda-196c287e221</a></p>
<p><a class="reference external" href="https://github.com/raffg/harry_potter_nlp">https://github.com/raffg/harry_potter_nlp</a></p>
<p><a class="reference external" href="https://bab2min.github.io/tomotopy/v0.12.2/en/">https://bab2min.github.io/tomotopy/v0.12.2/en/</a></p>
<p><a class="reference external" href="https://www.analyticsvidhya.com/blog/2021/06/part-18-step-by-step-guide-to-master-nlp-topic-modelling-using-lda-probabilistic-approach/">https://www.analyticsvidhya.com/blog/2021/06/part-18-step-by-step-guide-to-master-nlp-topic-modelling-using-lda-probabilistic-approach/</a>  the best…………………………………………………………………………………………..</p>
<p><a class="reference external" href="https://datascienceplus.com/topic-modeling-and-latent-dirichlet-allocation-lda/">https://datascienceplus.com/topic-modeling-and-latent-dirichlet-allocation-lda/</a></p>
<p>implementasi</p>
<p><a class="reference external" href="https://www.youtube.com/watch?v=BaM1uiCpj_E">https://www.youtube.com/watch?v=BaM1uiCpj_E</a></p>
<p><a class="reference external" href="https://www.baeldung.com/cs/topic-modeling-coherence-score">https://www.baeldung.com/cs/topic-modeling-coherence-score</a></p>
<p><a class="reference external" href="https://highdemandskills.com/topic-model-evaluation/">https://highdemandskills.com/topic-model-evaluation/</a></p>
<p>Pada tutorial ini, kita akan belajar tentang pemodelan topik, beberapa aplikasinya dan juga lebih mendalam dibahas salah satu model tentang teknik Latent Direchlet Allocation (LDA)</p>
<p>Untuk mempelajari tutorial ini diharapkan memahami terlebih dahulu distribusi probabilitas multivariate.</p>
<section id="pemodelan-topik">
<h2>2. Pemodelan Topik<a class="headerlink" href="#pemodelan-topik" title="Permalink to this headline">#</a></h2>
<section id="apa-itu-topic-modeling">
<h3>2.1. Apa itu  Topic Modeling?<a class="headerlink" href="#apa-itu-topic-modeling" title="Permalink to this headline">#</a></h3>
<div style="text-align: justify"> 
Dalam penambangan teks,  kumpulan dokumen, seperti posting blog atau artikel berita, yang telah dikumpulkan menjadi beberapa kelompok  sehingga kita dapat memahaminya secara terpisah. Pemodelan topik adalah metode untuk mengelompokkan dokumen dengan pembelajaran tak terawasi, mirip dengan pengelompokan pada data numerik, yang menemukan kelompok item pada suatu data  </div>
<div style="text-align: justify"> 
Ringkasnya, Topic modeling adalah metode pembelajaran terawasi dan inputnya adalah corpus dari suatu kumpulan dokumen dan untuk menghasilkan topik-topik tertentu. Setelah model topik dibuat, kita dapat menyatakan suatu dokumen sebagai kombinasi dari topik topik yang diperoleh.  Marilah lihat contoh berikut  </div>
<p><img alt="img" src="https://www.baeldung.com/wp-content/uploads/sites/4/2020/11/topic_modeling_1.png" /></p>
<p>Pada contoh ini, setiap dokumen dihubungkan dengan setiap topik sesuai dengan bobot-bobotnya. Oleh karena itu, kita dapat merepresentasikan dokumen sebagai vektor, dimana kolom-kolomnya adalah derajat keterkaitan dengan setiap topik.</p>
<p>Misalkan kita ingin memodelkan “Jurassic Park” sebagai kombinasi dari topik topik dalam proporsi seperti ini.</p>
<ul class="simple">
<li><p>Dinosaurs”: 97%</p></li>
<li><p>“Amusement Parks”: 49%</p></li>
<li><p>“Extinction”: 10%</p></li>
</ul>
<p>Dalam kasus tersebut, representasi vektor dokumen ini menjadi [0.97, 0.49, 0.1]</p>
<p>Apaa yang telah kita bahas adalah pemodelan topik yang  dapat digunakan untuk mendapatkan <strong>embedding vector</strong> dari  suatu dokumen.</p>
</section>
<section id="applications">
<h3>2.2. Applications<a class="headerlink" href="#applications" title="Permalink to this headline">#</a></h3>
<div class="math notranslate nohighlight">
\[
\text{perplexity}(\text{test set } w) =
        \exp \left\{
        - \frac{\mathcal L( w)}{\text{count of tokens}}
        \right\}
\]</div>
<p>Topic modeling memiliki beberapa bentuk aplikasi yang menarik diantaranya adalah :</p>
<ul class="simple">
<li><p>Memahami bidang</p></li>
<li><p>Menemukan perubahan atau trend baru dalam bidang kita</p></li>
<li><p>Melakukan Encode text sebagai  <a class="reference external" href="https://www.baeldung.com/cs/convert-word-to-vector">vector embeddings</a></p></li>
<li><p>Search Personalization</p></li>
<li><p>Pemodelan users’ preference untuk pencarian</p></li>
<li><p><a class="reference external" href="https://www.baeldung.com/cs/ml-similarities-in-text">Membandingkan dokumen</a></p></li>
<li><p>Menemukan penulis dari dokumen tertentu</p></li>
<li><p>Memprediksi dan menginterpretasikan aktifitas sosial and Interpretation of social activity</p></li>
<li><p><a class="reference external" href="https://www.baeldung.com/cs/sentiment-analysis-practical">Analisa Sentimen</a></p></li>
<li><p>Mesin penterjemah</p></li>
</ul>
</section>
<section id="teknik-pemodelan-topik">
<h3>2.3. Teknik Pemodelan Topik<a class="headerlink" href="#teknik-pemodelan-topik" title="Permalink to this headline">#</a></h3>
<p>terdapat beberapa teknik yang sering digunakan untuk pemodelan topik  diantaranya adalah</p>
<ul class="simple">
<li><p><a class="reference external" href="https://towardsdatascience.com/topic-modeling-articles-with-nmf-8c6b2a227a45">NMF</a> (Non-Negative Matrix Factorization)</p></li>
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Latent_semantic_analysis">LSA</a> (Latent Semantic Analysis)</p></li>
<li><p><a class="reference external" href="https://towardsdatascience.com/topic-modelling-with-plsa-728b92043f41">PLSA</a> (Probabilistic Latent Semantic Analysis)</p></li>
<li><p>LDA (Latent Dirichlet Allocation)</p></li>
<li><p><a class="reference external" href="https://paperswithcode.com/method/lda2vec">lda2vec</a></p></li>
<li><p><a class="reference external" href="https://paperswithcode.com/paper/tbert-topic-models-and-bert-joining-forces">tBERT</a> (Topic BERT)</p></li>
</ul>
</section>
</section>
<section id="latent-dirichlet-allocation">
<h2>3. Latent Dirichlet Allocation<a class="headerlink" href="#latent-dirichlet-allocation" title="Permalink to this headline">#</a></h2>
<section id="introduction">
<h3>3.1. Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">#</a></h3>
<p>Latent Dirichlet Allocation (LDA) adalah model a **statistical generative ** yang menggunkan Dirichlet distributions.</p>
<p>Algoritma ini daapat dipahami dalam dua karakteristik utuma</p>
<ul class="simple">
<li><p><strong>Every document is a mixture of topics</strong>. We imagine that each document may contain words from several topics in particular proportions. For example, in a two-topic model we could say “Document 1 is 90% topic A and 10% topic B, while Document 2 is 30% topic A and 70% topic B.”</p></li>
<li><p><strong>Every topic is a mixture of words</strong>. For example, we could imagine a two-topic model of American news, with one topic for “politics” and one for “entertainment.” The most common words in the politics topic might be “President”, “Congress”, and “government”, while the entertainment topic may be made up of words such as “movies”, “television”, and “actor”. Importantly, words can be shared between topics; a word like “budget” might appear in both equally.</p></li>
</ul>
<p>Kita mulai dengan corpus dari  <span class="math notranslate nohighlight">\(M\)</span>  dokumen dan berapak banyak topik <span class="math notranslate nohighlight">\(K\)</span> yang akan ditemukan dalam corpus-corpus tersebut. Keluaran dari topik model adalah <span class="math notranslate nohighlight">\(M\)</span> dokumen tersebut dinyatkan sebagai kombinasi dari <span class="math notranslate nohighlight">\(K\)</span> topik</p>
<p>Singkatnya, semua yang dilakukan algoritma algoritma pemodelan topik adalah menemukan bobot hubungan antara dokumen dan topik dan antara topik dan kata-kata</p>
<p>Ada tiga  <em>hyperparameter</em> pada  LDA:</p>
<ol class="simple">
<li><p><em>Faktor padat Dokumen-topik</em>  (<span class="math notranslate nohighlight">\(\alpha \)</span>) artinya proporsi topik-topik setiap dokumen</p></li>
<li><p><em>Faktor pada topik-kata</em> (<span class="math notranslate nohighlight">\(\beta\)</span>) artinya banyaknya kata-kata yang ada pada setiap topik</p></li>
<li><p><em>Banyaknya topik yang diharapkan</em>(<span class="math notranslate nohighlight">\(K\)</span>). artinya berapa banyak topik yang dihasilkan dari kumpulan dokumen-dokumen tersebut</p></li>
</ol>
<p>Hyperparameter <span class="math notranslate nohighlight">\(\alpha\)</span>  menentukan seberapa banyak subyek yang terkandung pada dokumen. Semakin rendah nilai <span class="math notranslate nohighlight">\(\alpha\)</span>  menunjukkan dokumen memilki beberapa subjek didalamnya, kemudian semakin tinggi nilai <span class="math notranslate nohighlight">\(\alpha\)</span> menunjukkan bahwa dokumen-dokumen tersebut akan memiliki banyak topik didalamnya.</p>
<p>Hyperparemeter <span class="math notranslate nohighlight">\(\beta\)</span> menentukan seberapa banyak distribusi kata pada masing masing subjek atau topik. Subyek dengan nilai <span class="math notranslate nohighlight">\(\beta\)</span> rendah biasanya memiliki beberapa kata, selanjutnya topik dengan nilai <span class="math notranslate nohighlight">\(\beta\)</span> tinggi akan memungkinkan memiliki banyak kata-kata.</p>
<p>Hyperparameter <span class="math notranslate nohighlight">\(K\)</span> menyatakan banyaknya topik yang ada pada kumpulan dokumen. <span class="math notranslate nohighlight">\(K\)</span> biasanya ditentukan nilainya berdasarkan pakar bidang tertentu. Pilihan lainnya  adalah untuk melatih model LDA dengan beberapa variasi dari nilai <span class="math notranslate nohighlight">\(K\)</span> kemudian menghitung  ‘Coherence Score.’</p>
<p>Berikut contoh pemodelan topik <em>Blei et al.</em> dimana kata-kata untuk setiap topik (arts, budgets, children, and education) . Teks yang berwarna pada bagian bawah dari gambar adalah menunjukkan suatu dokumen dengan terdiri dari beberapa kumpulan kata kata dari beberapa topik</p>
<figure class="align-default" id="directive-figa">
<a class="reference internal image-reference" href="../../_images/topikmodel.png"><img alt="../../_images/topikmodel.png" src="../../_images/topikmodel.png" style="height: 500px;" /></a>
<figcaption>
<p><span class="caption-text">Pemodelan Topik</span><a class="headerlink" href="#directive-figa" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Algoritma pemodelan topik <strong>Latent Dirichlet Allocation (LDA)</strong>, dikembangkan oleh <a class="reference external" href="http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf"><strong>Blei et al.</strong> </a>. Algoritma ini memiliki dua karakteristik</p>
<ul class="simple">
<li><p><strong>Setiap dokumen adalah gabungan dari topik-topik.</strong>. Kita dapat membayangkan bahwa setiap setiap dokumen mungkin berisi kata-kata dari beberapa topik sesuai proporsinya. Misalkan, dalam model dua topik, kita dapat mengatakan Dokumen 1 adalah 90% topik A dan 10% topik B, kemudian dokumen 2 adalah 30 % topik A dan 70% topik B</p></li>
<li><p><strong>Setiap tpik adalah gabungan dari kata kata</strong>. Misalkan kita dapat membayangkan model dua topik dengan satu topik politik dan satu topik lainnya ‘entartainment’.  Kata yang paling umum dalam topik politik mungkin  “President”, “Congress”, dan “government”, kemudian topik entertainment mungkin dibentuk dari kata kata seperti  “movies”, “television”, dan “actor”. Yang perlu diperhatikan, ada kata-kata dapat berada diantara kedua topik tersebut ; kata seperti  “budget” mungkin muncul pada kedua topik tersebut</p></li>
</ul>
</section>
</section>
<section id="kelebihan-dan-kekurangan-dari-lda">
<h2>Kelebihan dan kekurangan dari LDA<a class="headerlink" href="#kelebihan-dan-kekurangan-dari-lda" title="Permalink to this headline">#</a></h2>
<p>LDA adalah model probabilistik generatif dari corpus. Dibandingkan dengan metode pemodelan topik lainnya seperti model unigram, Latent Semantic Analysis (LSA), dan Probabilistic Latent Semantic Analysis (pLSA) kelebihan dan kekurangannya LDA adalah sebagai berikut:</p>
<p><strong>Keuntungan</strong></p>
<ul class="simple">
<li><p>Daoat menemukan topik tersirat (latent topik) didalam dokumen-dokumen</p></li>
<li><p>Pembelajaran terawasi  membutuhkan label sesungguhnya, yang mungkin tidak tersedia</p></li>
<li><p>LDA mudah untuk dilatih</p></li>
<li><p>LDA dapat dipasangkan dengan word2vec untuk mempertahankan representasi kata.</p></li>
<li><p>LDA memberikan topik yang dapat ditafsirkan</p></li>
</ul>
<p><strong>Kelemahan</strong></p>
<ul class="simple">
<li><p>Hanya menganggap dokumen sebagai kantong kata (bag of word) dan mengabaikan informasi sintaksis (misalnya urutan kata) dan informasi semantik (misalnya makna lain dari  kata tertentu)</p></li>
<li><p>Banyaknya topik tetap</p></li>
<li><p>Topik-topik tidak saling berkaitan ( Distribusi topik Dirichlet tidak dapat tidak dapat menangkap korelasi)</p></li>
<li><p>Statik( tidak ada perubahan topik dari waktu ke waktue)</p></li>
</ul>
<p>Kita akan masuk ke konsep pertama. LDA tidak memberikan jawaban yang jelas apakah suatu dokumen termasuk dalam topik tertentu karena dokumen dianggap sebagai campuran topik. Dalam satu dokumen, kita mungkin menemukan beberapa topik. Sebuah berita tentang krisis keuangan tahun 2008 dapat terdiri dari topik ekonomi, politik dan sosial, digabung dalam satu artikel yang membahas tentang nilai pasar, respon pemerintah dan dampak dari tingkat pengangguran yang tinggi. Sehingga misalkan dokument tersebut dapat terdiri dari 30% ekonomi, 60% politik, dan 10% topik sosial. Persentase masing-masing topik pada dokumen mewakili probabilitas dokumen terhadap topik tertentu Oleh karena itu, kita dapat membayangkan gabungan topik sebagai distribusi probabilitas seperti tabel berikut:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>news</p></th>
<th class="head"><p>economics</p></th>
<th class="head"><p>politics</p></th>
<th class="head"><p>social</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p>0.1437888</p></td>
<td><p>0.02277825</p></td>
<td><p>0.8334330</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p>0.3941526</p></td>
<td><p>0.26405274</p></td>
<td><p>0.3417947</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p>0.2044885</p></td>
<td><p>0.44620952</p></td>
<td><p>0.3493020</p></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p>0.4415087</p></td>
<td><p>0.27571751</p></td>
<td><p>0.2827738</p></td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><p>0.4702336</p></td>
<td><p>0.22830737</p></td>
<td><p>0.3014590</p></td>
</tr>
</tbody>
</table>
<p>Baris pertama dari berita 14% berpeluang ke berita  ekonomi , 2% berita politik dan 83% berita sosial. baris kedua memilik kemungkinan  39% berita ekonomi, 26% berita politik dan 34% berita sosial dan seterusnya. Semakin tinggi peluangnya berarti dokumen tersebut  news and so on. Peluang yang lebih tinggi berarti bahwa dokumen dapat diwakili oleh suatu topik .</p>
<p>Kata Latent di Latent Dirichlet Allocation mengacu pada struktur laten atau tersembunyi di dalam dokumen. Seperti yang telah kita lihat di bagian sebelumnya, kita secara jelas mengatakan  bahwa sekelompok kata dapat memiliki tema atau topik utama. Oleh karena itu, menurut prinsip kedua, topik adalah campuran kata-kata, di mana kata-kata tertentu memiliki asosiasi yang kuat dengan topik tertentu. Misalnya, kata Presiden memiliki sinyal kuat bahwa itu termasuk topik politik, atau kata Pinjaman dan Bunga memiliki sinyal kuat untuk topik ekonomi. Sama seperti sebuah dokumen memiliki distribusi probabilitas untuk setiap topik, sebuah topik juga memiliki distribusi probabilitas untuk setiap kata/istilah. LDA menganggap bahwa sebuah dokumen adalah kumpulan kata, sehingga kita tidak memperdulikan urutan kata.</p>
<p>Model LDA model dientuk berdasarkan dua prinsip ini dan memiliki struktur seperti berikut:</p>
<figure class="align-default" id="id1">
<a class="reference internal image-reference" href="../../_images/ldastruc.jpg"><img alt="../../_images/ldastruc.jpg" src="../../_images/ldastruc.jpg" style="height: 400px;" /></a>
<figcaption>
<p><span class="caption-text">Pemodelan Topik LDA</span><a class="headerlink" href="#id1" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Notation:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\alpha\)</span> = Paramter dirichlet untuk proporsi topi per-dokumen</p></li>
<li><p><span class="math notranslate nohighlight">\(\theta_d\)</span> = Distribusi topik dari dokumen <span class="math notranslate nohighlight">\(d\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(Z_{d,n}\)</span> = Tanda suatu topik dengan kata ke <span class="math notranslate nohighlight">\(n\)</span> pada dokumen <span class="math notranslate nohighlight">\(d\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(W_{d,n}\)</span> = Kata yang diamati dari kata ke-<span class="math notranslate nohighlight">\(n\)</span> dalam dokumen <span class="math notranslate nohighlight">\(d\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\eta\)</span> = hyperparameter topik</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta_k\)</span>  = Distribusi kata-kata dari topik <span class="math notranslate nohighlight">\(k\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(K\)</span> = Kumpulan topik-topik</p></li>
<li><p><span class="math notranslate nohighlight">\(N\)</span> = Kumapulan kata dalam dokumen</p></li>
<li><p><span class="math notranslate nohighlight">\(D\)</span> = Kumpulan dokumen di corpus</p></li>
</ul>
<p>LDA merupakan bagian  model bayesian hierarkis. Model bayesian memiliki prior dan posterior. Prior berarti distribusi probabilitas dari hal-hal tertentu sebelum kita melihat datanya. Sebagai contoh, kita mungkin percaya bahwa distribusi probabilitas untuk pelemparan dadu terdistribusi secara seragam, memiliki probabilitas kemunculan sama pada permukaan mata dadu tersebut. Kami mengasumsikan  ini  melempar dadu. Sedangkan probabilitas posterior mencerminkan distribusi probabilitas setelah kita  melempar dadu. Misalnya, setelah kita melempar dadu sebanyak 10.000 kali, angka 5 muncul lebih dari 5.000 kali. Ini dapat mengubah keyakinan kita bahwa nilai untuk setiap sisi berdistribusi  secara merata, dikarenakan ada satu  satu sisi dadu memiliki peluang tinggi kemunculannya.  Kita dapat menghitung probabilitas posterior menggunakan Teorema Bayes.  Konsep probabilitas prior dan posterior sangat penting untuk memahami proses LDA.</p>
<p>Model LDA terdiri dari 2 macam distribusi prior : distribusi probabilitas topik ke dokumen (probabilitas topik-dokumen) dan distribusi probabilitas kata ke topik (probabilitas topik-kata). Kedua Distribusi probabilitas prior tersebut tidak seragam atau berdistribusi normal, tetapi berdistribusi Dirichlet dengan paramete <span class="math notranslate nohighlight">\(\alpha\)</span>  dan parameter <span class="math notranslate nohighlight">\(\beta\)</span>  dan juga jumlah topik sebagai inputan.</p>
<p>Gambar berikut adalah ilustrasi distribusi Dirichlet untuk setiap topik pada setiap dokumen. Misalkan kita memiliki kumpulan dokumen dan kita yakin bahwa pada corpus kita memiliki 3 topik yang berbeda. Setiap titik mewakili satu dokumen dan setiap sisi segitiga mewakili satu topik. Posisi setiap dokumen mewakili distribusi probabilitas untuk setiap topik. Kami secara acak menetapkan posisi setiap dokumen berdasarkan distribusi probabilitasnya. Kita dapat melihat bahwa dokumen A memiliki kedekatan yang dekat dengan topik Sains. Hal ini menunjukkan bahwa dokumen A memiliki probabilitas tinggi untuk masuk ke topik Sains dan probabilitas rendah untuk masuk ke Politik atau Ekonomi. Sementara itu, dokumen B memiliki probabilitas tinggi untuk masuk dalam topik Ekonomi. Dokumen C terletak di antara Sains dan Politik, sehingga memiliki probabilitas yang agak sama untuk topik tersebut dan probabilitas rendah untuk topik Ekonomi. Topik D terletak di tengah segitiga, yang berarti memiliki peluang yang sama untuk ketiga topik.</p>
<figure class="align-default" id="id2">
<a class="reference internal image-reference" href="../../_images/iluslda.png"><img alt="../../_images/iluslda.png" src="../../_images/iluslda.png" style="height: 400px;" /></a>
<figcaption>
<p><span class="caption-text">Pemodelan Topik prior LDA</span><a class="headerlink" href="#id2" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Tujuan LDA adalah untuk menjadikan setiap dokumen sedekat mungkin dengan topik tertentu, meskipun mungkin tidak selalu tercapai untuk semua dokumen. Jika gambar di atas menyatakan probabilitas prior untuk setiap dokumen untuk setiap topik, gambar berikut menggambarkan probabilitas posterior di mana semua dokumen memiliki hubungan yang kuat dengan topik tertentu. ilusldapos</p>
<figure class="align-default" id="id3">
<a class="reference internal image-reference" href="../../_images/ilusldapos.png"><img alt="../../_images/ilusldapos.png" src="../../_images/ilusldapos.png" style="height: 400px;" /></a>
<figcaption>
<p><span class="caption-text">Pemodelan Topik Posterior LDA</span><a class="headerlink" href="#id3" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Kita jadikan distribusi probabilitasnya kedalam tabel berikut:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>document</p></th>
<th class="head"><p>sciences</p></th>
<th class="head"><p>politics</p></th>
<th class="head"><p>economy</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>A</p></td>
<td><p>0.8</p></td>
<td><p>0.10</p></td>
<td><p>0.10</p></td>
</tr>
<tr class="row-odd"><td><p>B</p></td>
<td><p>0.1</p></td>
<td><p>0.05</p></td>
<td><p>0.85</p></td>
</tr>
<tr class="row-even"><td><p>C</p></td>
<td><p>0.1</p></td>
<td><p>0.85</p></td>
<td><p>0.05</p></td>
</tr>
<tr class="row-odd"><td><p>D</p></td>
<td><p>0.1</p></td>
<td><p>0.80</p></td>
<td><p>0.10</p></td>
</tr>
</tbody>
</table>
<p>Distribusi prior kedua adalah probabilitas suatu topik terhadap kata kata tertentu yang dimiliki: probabilitas kata-topik. Prior ini berkaitan dengan kata-kata apa yang mewakili topik tertentu? Apakah ada kata-yang memiliki hubungan kuat dengan topik tertentu? Katakanlah kita memiliki 4 istilah: Presiden, Planet, Pasar, dan Energi. Setiap kata akan bertindak sebagai tepi segitiga. Namun, karena kita memiliki lebih dari 3 suku, bentuk distribusinya bukan segitiga lagi, melainkan tetrahedron. Gambar berikut mengilustrasikan secara acak dengan distribusi Dirichlet antara topik dan kata-kata:</p>
<figure class="align-default" id="id4">
<a class="reference internal image-reference" href="../../_images/ilusldtopkata.png"><img alt="../../_images/ilusldtopkata.png" src="../../_images/ilusldtopkata.png" style="height: 400px;" /></a>
<figcaption>
<p><span class="caption-text">Pemodelan Topik-kata LDA</span><a class="headerlink" href="#id4" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Some of the well-known topic modelling techniques are Latent Semantic Analysis (LSA), Probabilistic Latent Semantic Analysis (PLSA), Latent Dirichlet Allocation (LDA), and Correlated Topic Model (CTM). In this article, we will focus on LDA, a popular topic modelling technique.</p>
<p>Contents [<a class="reference external" href="https://petamind.com/understanding-latent-dirichlet-allocation-lda/">hide</a>]</p>
<ul class="simple">
<li><p><a class="reference external" href="https://petamind.com/understanding-latent-dirichlet-allocation-lda/#Latent_Dirichlet_Allocation_LDA">1 Latent Dirichlet Allocation (LDA)</a></p></li>
<li><p><a class="reference external" href="https://petamind.com/understanding-latent-dirichlet-allocation-lda/#LDA_Algorithm">2 LDA Algorithm</a></p></li>
<li><p><a class="reference external" href="https://petamind.com/understanding-latent-dirichlet-allocation-lda/#Hyper_parameters_in_LDA">3 Hyper parameters in LDA</a></p></li>
<li><p><a class="reference external" href="https://petamind.com/understanding-latent-dirichlet-allocation-lda/#Coherence_score_Topic_Coherence_score">4 Coherence score/ Topic Coherence score</a></p></li>
<li><p><a class="reference external" href="https://petamind.com/understanding-latent-dirichlet-allocation-lda/#Data_preprocessing_for_LDA">5 Data preprocessing for LDA</a></p></li>
<li><p>6 Some real-world novel applications of LDA</p>
<ul>
<li><p><a class="reference external" href="https://petamind.com/understanding-latent-dirichlet-allocation-lda/#Share_this">6.1 Share this:</a></p></li>
</ul>
</li>
</ul>
</section>
<section id="latent-dirichlet-allocation-lda">
<h2><strong>LATENT DIRICHLET ALLOCATION (LDA)</strong><a class="headerlink" href="#latent-dirichlet-allocation-lda" title="Permalink to this headline">#</a></h2>
<p>Before getting into the details of the <strong>Latent Dirichlet Allocation</strong> model, let’s look at the words that form the name of the technique. The word <strong>‘Latent’</strong> indicates that the model discovers the ‘yet-to-be-found’ or hidden topics from the documents. <strong>‘Dirichlet’</strong> indicates LDA’s assumption that the distribution of topics in a document and the distribution of words in topics are both <a class="reference external" href="https://en.wikipedia.org/wiki/Dirichlet_distribution">Dirichlet distributions</a>. ‘<strong>Allocation’</strong> indicates the distribution of topics in the document.</p>
<p>LDA assumes that documents are composed of words that help determine the topics and maps documents to a list of topics by assigning each word in the document to different topics. The assignment is in terms of conditional probability estimates as shown in figure 2. In the figure, the value in each cell indicates the probability of a word <em>wj</em> belonging to topic <em>tk</em>. ‘<em>j</em>’ and ‘<em>k</em>’ are the word and topic indices respectively. It is important to note that LDA ignores the order of occurrence of words and the syntactic information. It treats documents just as a collection of words or a bag of words.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./contents/Webmining"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Mulaab-UTM<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>